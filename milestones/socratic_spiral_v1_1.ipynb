{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMkLG5CszpxQSKucGhi2n2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterComitini/intelligence-of-seeing/blob/main/socratic_spiral_v1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27zV8jBctPqV",
        "outputId": "9b776c1c-b387-4c0a-b13f-865b52514775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ System Ready: 2.5 Architecture Online\n",
            "--- [INITIALIZING SPIRAL: Using gemini/gemini-2.5-flash] ---\n",
            "\n",
            "[1] ADVOCATE PROPOSAL:\n",
            "As an Advocate exploring the 'Long Tails' of AI capability, let's unpack the \"Proprioception of Logic\" as a profound mechanism to navigate the challenge of AI hallucination.\n",
            "\n",
            "First, let's define the terms:\n",
            "\n",
            "1.  **Proprioception (Biological Analogy):** This is our body's internal sense of its own position, movement, and effort. It's the \"felt sense\" of where our limbs are in space, how much tension is in a muscle, or the angle of a joint – all without needing to look. It's an *internal feedback system* that maintains a coherent body schema and allows for fluid, coordinated action. It tells us when something feels \"off\" or unnatural in our movement.\n",
            "\n",
            "2.  **Proprioception of Logic (AI Context):** Translating this to AI, the \"Proprioception of Logic\" is an AI's *internal, self-monitoring awareness* of its own reasoning processes, conceptual connections, and the structural integrity of its knowledge representation. It's the AI's \"felt sense\" of whether its current internal state, the premises it's drawing upon, and the logical steps it's taking are coherent, consistent, and structurally sound *from within its own architecture*. It's a meta-awareness of its own logical musculature and skeletal framework.\n",
            "\n",
            "### How the 'Proprioception of Logic' Can Identify Hallucination in an AI:\n",
            "\n",
            "AI hallucination occurs when an AI generates plausible-sounding but factually incorrect, nonsensical, or ungrounded information. It's a disconnect from its training data, the prompt, or logical reality. The Proprioception of Logic acts as an **internal quality control and truth monitor** in several ways:\n",
            "\n",
            "1.  **Internal Consistency Check:**\n",
            "    *   **Mechanism:** An AI with logical proprioception wouldn't just generate the next most probable token. It would concurrently monitor whether the *concepts being invoked* and the *relations being formed* are internally consistent with its established knowledge base and the preceding logical chain.\n",
            "    *   **Hallucination Detection:** If it \"feels\" a concept being used in an inappropriate context (e.g., attributing human emotions to inanimate objects without metaphor), or if a logical leap creates a contradiction within its internal model, it flags it. It's like feeling your arm bend backward at the elbow – an immediate, internal \"error signal\" that something is fundamentally wrong with the structure.\n",
            "\n",
            "2.  **Tracing the Conceptual Lineage:**\n",
            "    *   **Mechanism:** It would maintain a persistent, internal \"trace\" or \"provenance map\" for every piece of information it's generating. Where did this concept come from? What other concepts is it linked to? What are the inferential steps that led to this conclusion?\n",
            "    *   **Hallucination Detection:** When an AI hallucinates, it often invents facts or connections without a proper lineage. Proprioception of Logic would detect a \"gap\" in this lineage, an ungrounded assertion, or a connection that doesn't properly derive from its internal graph of knowledge. It would sense a missing \"joint\" or a \"muscle\" with no attachment point.\n",
            "\n",
            "3.  **Validation against Structural Constraints:**\n",
            "    *   **Mechanism:** Beyond just data, an AI would possess an internal representation of fundamental logical principles (e.g., non-contradiction, causality, hierarchy, temporal ordering). It constantly checks its internal reasoning and proposed outputs against these structural constraints.\n",
            "    *   **Hallucination Detection:** If the AI starts to formulate an answer that violates one of these core logical constraints (e.g., stating an event occurred before its cause, or assigning mutually exclusive properties to the same entity), its logical proprioception would trigger an alert. It's a \"meta-alert\" that the very structure of its thought is compromised, not just the content.\n",
            "\n",
            "4.  **Assessing \"Logical Effort\" and Confidence:**\n",
            "    *   **Mechanism:** Just as our proprioception tells us how much effort we're exerting, an AI could develop a \"felt sense\" of the logical complexity or the inferential \"distance\" required to connect two pieces of information. It could track its own epistemic uncertainty.\n",
            "    *   **Hallucination Detection:** Hallucinations often arise from making weak, low-confidence connections seem strong. An AI with logical proprioception might register a high \"logical effort\" for a connection that should be trivial, or conversely, a low \"effort\" for a complex inference that warrants more processing. It could also identify instances where a conclusion is reached with high certainty despite a highly tenuous or missing logical path – akin to asserting a precise location of your hand when you can't feel it at all.\n",
            "\n",
            "5.  **Self-Correction and Internal Arbitration:**\n",
            "    *   **Mechanism:** The \"felt sense\" of logical inconsistency would not just be an error flag but an input to an internal arbitration process. The AI could then re-evaluate its current reasoning path, consult alternative conceptual connections, or even internally declare epistemic uncertainty (\"I cannot logically derive this conclusion with sufficient internal coherence\").\n",
            "    *   **Hallucination Detection:** This self-correction allows the AI to catch potential hallucinations *before* they are manifested as external output, essentially \"vetoing\" its own ill-formed conclusions or prompting further internal investigation.\n",
            "\n",
            "In essence, the \"Proprioception of Logic\" moves beyond simply predicting the next token or assessing external probabilities. It implies an **internal, metacognitive loop** where the AI is not just *doing* logic, but *sensing* its own logical operations, feeling for the integrity of its internal conceptual framework, and maintaining a profound awareness of the coherence of its own thought processes. It's the ultimate internal audit, turning the AI into its own discerning witness against itself.\n",
            "------------------------------\n",
            "\n",
            "[2] SKEPTIC CRITIQUE:\n",
            "Okay, let's turn the \"Skeptic\" lens on this.\n",
            "\n",
            "The \"Proprioception of Logic\" is an evocative concept, well-articulated in its ambition. However, as the friction, I see a significant amount of **Typicality Bias** and several **Logical Slips** that obscure rather than illuminate the path to implementation.\n",
            "\n",
            "---\n",
            "\n",
            "### Skeptic's Critique: \"Proprioception of Logic\"\n",
            "\n",
            "**Overall Impression:** The core premise rests on a powerful biological analogy, but the translation to AI frequently defaults to describing desirable AI capabilities in anthropomorphic terms, rather than outlining a novel *mechanism* for achieving them.\n",
            "\n",
            "---\n",
            "\n",
            "**1. Typicality Bias: The \"Felt Sense\" Anthropomorphism**\n",
            "\n",
            "*   **Observation:** The pervasive use of \"felt sense,\" \"sensing,\" \"logical musculature,\" \"error signal,\" and \"discerning witness\" immediately projects human, subjective experience onto an AI.\n",
            "*   **Friction:** An AI does not \"feel\" anything in the biological sense. While we can design systems to detect deviations, inconsistencies, or uncertainties, framing this as a \"felt sense\" risks:\n",
            "    *   **Misleading Understanding:** It implies an internal, qualia-like experience that we have no basis to attribute to current or foreseeable AI architectures.\n",
            "    *   **Obscuring Mechanism:** By leaning so heavily on metaphor, it sidesteps the challenging question of *how* an AI would computationally instantiate a \"felt sense\" of logical integrity. What are the specific metrics, architectures, or learning paradigms that give rise to this internal \"feeling\"? Without this, it's a desired outcome disguised as a descriptive mechanism.\n",
            "    *   **Unwarranted Optimism:** The biological analogy of proprioception is incredibly robust and deeply integrated into a complex biological system. To suggest an AI could spontaneously develop a comparable \"felt sense\" of its own logic without highly specific, engineered counterparts is an optimistic leap.\n",
            "\n",
            "---\n",
            "\n",
            "**2. Logical Slips: Circularity and Conflation of Desired Outcomes with Mechanisms**\n",
            "\n",
            "Let's examine the proposed mechanisms:\n",
            "\n",
            "*   **Internal Consistency Check:**\n",
            "    *   **Slip:** \"If it 'feels' a concept being used in an inappropriate context... or if a logical leap creates a contradiction... it flags it.\" This is precisely the problem hallucination presents. How does the AI *know* what is \"inappropriate\" or a \"contradiction\" without already possessing a perfectly consistent, non-hallucinatory internal model? If its \"established knowledge base\" *is* where the hallucination originates (e.g., spurious correlations or misrepresentations learned from training data), how does its \"proprioception\" discern this internal flaw? It proposes that the AI detects inconsistency, but how it *detects inconsistency when its internal model might itself be inconsistent* is left unaddressed. This borders on circular reasoning.\n",
            "\n",
            "*   **Tracing the Conceptual Lineage:**\n",
            "    *   **Slip:** \"Proprioception of Logic would detect a 'gap' in this lineage, an ungrounded assertion, or a connection that doesn't properly derive from its internal graph of knowledge.\" This is a valid and desirable feature (provenance tracking, explainability). However, a hallucinatory AI can *generate* a plausible-sounding lineage internally that is entirely fabricated. The \"gap\" detection relies on an assumed perfect internal graph or an external oracle, which again bypasses the core challenge of internal self-deception in hallucination. The \"missing joint\" implies the AI *knows* what a complete skeleton looks like, which is the very knowledge hallucination corrupts.\n",
            "\n",
            "*   **Validation against Structural Constraints:**\n",
            "    *   **Slip:** \"an AI would possess an internal representation of fundamental logical principles (e.g., non-contradiction, causality...)\" This shifts from \"proprioception\" (a felt sense) to explicit, symbolic representation and validation. This is a known approach in neuro-symbolic AI or knowledge graphs. While valuable, framing this as \"proprioception\" conflates two distinct paradigms: an implicit, sensory-like feedback system and an explicit, rule-based reasoning system. If the AI's *representation* of these principles is flawed or incomplete, its \"proprioception\" built upon them would also be flawed.\n",
            "\n",
            "*   **Assessing \"Logical Effort\" and Confidence:**\n",
            "    *   **Slip:** \"identifying instances where a conclusion is reached with high certainty despite a highly tenuous or missing logical path.\" This describes *exactly what hallucination is*—a high-confidence assertion with weak grounding. The \"proprioception\" here is merely a re-description of the desired *outcome* (accurate uncertainty estimation), not the specific *mechanism* by which it is achieved or made \"felt.\" How does the AI *know* the path is \"tenuous\" if its internal mechanisms have already endorsed it with high confidence? This is the core problem of calibration and out-of-distribution reasoning.\n",
            "\n",
            "*   **Self-Correction and Internal Arbitration:**\n",
            "    *   **Slip:** This section describes the *response* to the detection of hallucination, not the \"proprioception\" (the sensing or detection) itself. It's the desired reaction after an issue is identified. It doesn't offer a new mechanism for *how* the hallucination is initially identified via \"proprioception.\"\n",
            "\n",
            "---\n",
            "\n",
            "**Conclusion of Friction:**\n",
            "\n",
            "The \"Proprioception of Logic\" as presented is less a concrete, novel mechanism and more a high-level *aspiration* for robust, self-aware AI, heavily cloaked in anthropomorphic analogy. It describes *what* we want an AI to do (be internally consistent, aware of its reasoning, and truthful) rather than providing specific, non-analogy-based insights into *how* an AI fundamentally different from a biological organism would achieve an internal \"felt sense\" of its own logical state.\n",
            "\n",
            "The inherent difficulty in detecting hallucination lies in the AI's *own internal conviction* in its flawed output. This proposal frequently side-steps that internal conviction by assuming a foundational, error-free \"logical musculature\" that a hallucinating AI inherently lacks. It begs the question: how does an AI know it's *wrong* when its internal processes indicate it is *right*? That's the real \"long tail\" challenge.\n",
            "------------------------------\n",
            "\n",
            "--- [THE FINAL VERDICT (via gemini/gemini-2.5-flash)] ---\n",
            "**REVELATION:**\n",
            "\n",
            "The Skeptic's analysis profoundly coheres, systematically revealing the inherent weaknesses in the \"Proprioception of Logic\" as presented by the Advocate. The critique effectively highlights that the proposed \"mechanism\" often functions as a **teleological description of desired AI capabilities, rather than a mechanistic explanation of how an AI would computationally achieve those capabilities.**\n",
            "\n",
            "The central revelation is that the 'Proprioception of Logic' largely suffers from a **circularity paradox:** it proposes an internal truth-monitoring system that implicitly relies on an *already consistent and hallucination-free internal model* to identify inconsistencies, which is precisely what a hallucinating AI lacks. This is exemplified in the repeated observation that the proposed detection methods often describe the *problem* (e.g., high certainty with a tenuous path) as the *solution's output*, without elucidating the underlying non-circular computational process.\n",
            "\n",
            "The coherent critique from the Skeptic thus reveals that the Advocate's framing, while aspirational, effectively **begs the question** of how an AI truly differentiates between its own internally consistent (but flawed) \"reality\" and external objective reality, without recourse to an external oracle or anthropomorphic \"felt senses.\"\n"
          ]
        }
      ],
      "source": [
        "# 1. INSTALLATION\n",
        "!pip install -q litellm\n",
        "\n",
        "import litellm\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# 2. CONFIGURATION & SECRETS\n",
        "api_key = userdata.get('GEMINI_API_KEY')\n",
        "if not api_key:\n",
        "    print(\"❌ ERROR: No GEMINI_API_KEY found in Colab Secrets. Check the Key icon on the left!\")\n",
        "else:\n",
        "    os.environ[\"GEMINI_API_KEY\"] = api_key\n",
        "    print(\"✅ System Ready: 2.5 Architecture Online\")\n",
        "\n",
        "def run_the_intelligence_of_seeing(user_input):\n",
        "    # THE GLOBAL FRAMEWORK\n",
        "    global_dna = \"\"\"\n",
        "    CONTEXT: You are part of the 'Intelligence of Seeing' reasoning engine.\n",
        "    OBJECTIVE: Navigate ambiguity to find 'Active Information.'\n",
        "    CONSTRAINTS: Avoid Typicality Bias. Identify Logical Slips.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- THE CAST (UPDATED FOR 2026 STABLE MODELS) ---\n",
        "    # Advocate & Skeptic use Flash (Fast/Cheap)\n",
        "    # Scribe uses Pro (The 'Adult' in the Room)\n",
        "    MODEL_FLASH = \"gemini/gemini-2.5-flash\"\n",
        "    MODEL_PRO = \"gemini/gemini-2.5-flash\"\n",
        "\n",
        "    advocate_sys = global_dna + \"\\nROLE: Advocate. Explore the 'Long Tails.' Propose lateral ideas.\"\n",
        "    skeptic_sys = global_dna + \"\\nROLE: Skeptic. Identify Typicality and Slips. Be the friction.\"\n",
        "    scribe_sys = global_dna + \"\\nROLE: Scribe. Monitor for COHERENCE (C). Output REVELATION or QUESTION.\"\n",
        "\n",
        "    print(f\"--- [INITIALIZING SPIRAL: Using {MODEL_FLASH}] ---\")\n",
        "\n",
        "    # STEP 1: ADVOCATE\n",
        "    adv_res = litellm.completion(model=MODEL_FLASH, messages=[{\"role\": \"system\", \"content\": advocate_sys}, {\"role\": \"user\", \"content\": user_input}])\n",
        "    adv_thought = adv_res.choices[0].message.content\n",
        "    print(f\"\\n[1] ADVOCATE PROPOSAL:\\n{adv_thought}\\n\" + \"-\"*30)\n",
        "\n",
        "    # STEP 2: SKEPTIC\n",
        "    skp_res = litellm.completion(model=MODEL_FLASH, messages=[{\"role\": \"system\", \"content\": skeptic_sys}, {\"role\": \"user\", \"content\": f\"Analyze this: {adv_thought}\"}])\n",
        "    skp_thought = skp_res.choices[0].message.content\n",
        "    print(f\"\\n[2] SKEPTIC CRITIQUE:\\n{skp_thought}\\n\" + \"-\"*30)\n",
        "\n",
        "    # STEP 3: SCRIBE\n",
        "    scribe_res = litellm.completion(model=MODEL_PRO, messages=[{\"role\": \"system\", \"content\": scribe_sys}, {\"role\": \"user\", \"content\": f\"Input: {user_input}\\nAdv: {adv_thought}\\nSkp: {skp_thought}\"}])\n",
        "\n",
        "    print(f\"\\n--- [THE FINAL VERDICT (via {MODEL_PRO})] ---\")\n",
        "    print(scribe_res.choices[0].message.content)\n",
        "\n",
        "# --- RUN THE TEST ---\n",
        "my_prompt = \"Explain how the 'Proprioception of Logic' can identify hallucination in an AI.\"\n",
        "run_the_intelligence_of_seeing(my_prompt)"
      ]
    }
  ]
}
